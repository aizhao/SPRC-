# RoI Alignå®ç°æ€»ç»“

## ğŸ“‹ å®ç°æ¦‚è¿°

å·²æˆåŠŸå°†FG-CLIPçš„RoI AlignåŒºåŸŸç‰¹å¾æå–æ¨¡å—é›†æˆåˆ°SPRCé¡¹ç›®ä¸­ï¼Œç”¨äºæå‡CIRRæ•°æ®é›†ä¸Šçš„ç»†ç²’åº¦å›¾åƒæ£€ç´¢æ€§èƒ½ã€‚

## âœ… å®Œæˆçš„å·¥ä½œ

### 1. æ¨¡å‹ä¿®æ”¹
**æ–‡ä»¶**: `/home/caoyu/mnt/zhaoai/SPRC/src/lavis/models/blip2_models/blip2_qformer_cir_align_prompt.py`

#### æ–°å¢åŠŸèƒ½:
- âœ… å¯¼å…¥ `torchvision.ops.roi_align`
- âœ… æ·»åŠ  `region_proj` æŠ•å½±å±‚ï¼ˆç¬¬96è¡Œï¼‰
- âœ… æ·»åŠ  `use_region_loss` æ§åˆ¶æ ‡å¿—ï¼ˆç¬¬97è¡Œï¼‰
- âœ… å®ç° `extract_region_features()` æ–¹æ³•ï¼ˆç¬¬304-362è¡Œï¼‰
  - ä»ViT patchç‰¹å¾é‡æ„feature map
  - ä½¿ç”¨RoI Alignæå–åŒºåŸŸç‰¹å¾
  - æ”¯æŒæ‰¹å¤„ç†å’Œå¯å˜æ•°é‡çš„boxes
- âœ… å®ç° `compute_region_loss()` æ–¹æ³•ï¼ˆç¬¬364-416è¡Œï¼‰
  - è®¡ç®—å‚è€ƒå›¾åƒå’Œç›®æ ‡å›¾åƒçš„åŒºåŸŸçº§å¯¹æ¯”æŸå¤±
  - æ”¯æŒä¸åŒæ•°é‡çš„boxes
  - å¤„ç†ç©ºboxesæƒ…å†µ
- âœ… ä¿®æ”¹ `forward()` æ–¹æ³•ï¼ˆç¬¬207-215è¡Œï¼‰
  - é›†æˆåŒºåŸŸæŸå¤±åˆ°è®­ç»ƒæµç¨‹
  - è¿”å›åŒ…å«åŒºåŸŸæŸå¤±çš„å­—å…¸

### 2. è®­ç»ƒè„šæœ¬ä¿®æ”¹
**æ–‡ä»¶**: `/home/caoyu/mnt/zhaoai/SPRC/src/blip_fine_tune_2.py`

#### æ–°å¢åŠŸèƒ½:
- âœ… æ·»åŠ å‘½ä»¤è¡Œå‚æ•° `--use-region-loss`ï¼ˆç¬¬386-387è¡Œï¼‰
- âœ… æ·»åŠ å‘½ä»¤è¡Œå‚æ•° `--loss-region`ï¼ˆç¬¬385è¡Œï¼‰
- âœ… åœ¨è®­ç»ƒåˆå§‹åŒ–æ—¶å¯ç”¨åŒºåŸŸæŸå¤±ï¼ˆç¬¬230-233è¡Œï¼‰
- âœ… åœ¨æŸå¤±è®¡ç®—ä¸­æ·»åŠ åŒºåŸŸæŸå¤±æƒé‡ï¼ˆç¬¬293-302è¡Œï¼‰
- âœ… å°†å‚æ•°ä¼ é€’åˆ°è®­ç»ƒé…ç½®ï¼ˆç¬¬419-420è¡Œï¼‰

### 3. æ–‡æ¡£å’Œæµ‹è¯•
- âœ… åˆ›å»ºè¯¦ç»†ä½¿ç”¨æ–‡æ¡£ `ROI_ALIGN_USAGE.md`
- âœ… åˆ›å»ºæµ‹è¯•è„šæœ¬ `test_roi_align.py`ï¼ˆåŒ…å«5ä¸ªæµ‹è¯•ç”¨ä¾‹ï¼‰
- âœ… åˆ›å»ºè®­ç»ƒç¤ºä¾‹è„šæœ¬ `train_with_roi_align.sh`
- âœ… åˆ›å»ºå®ç°æ€»ç»“æ–‡æ¡£ï¼ˆæœ¬æ–‡æ¡£ï¼‰

## ğŸ¯ æ ¸å¿ƒä»£ç ç‰‡æ®µ

### 1. RoI Alignç‰¹å¾æå–
```python
def extract_region_features(self, image_embeds, boxes, image_size=(224, 224)):
    """ä½¿ç”¨RoI Alignä»å›¾åƒç‰¹å¾ä¸­æå–åŒºåŸŸç‰¹å¾"""
    batch_size = image_embeds.shape[0]
    hidden_dim = image_embeds.shape[-1]
    feature_map_size = int(image_embeds.shape[1] ** 0.5)  # 14x14
    
    all_region_features = []
    for i in range(batch_size):
        if boxes[i] is None or len(boxes[i]) == 0:
            all_region_features.append(torch.empty(0, hidden_dim, device=image_embeds.device))
            continue
        
        # Reshape to feature map
        feat_map = image_embeds[i].view(feature_map_size, feature_map_size, hidden_dim)
        feat_map = feat_map.permute(2, 0, 1).unsqueeze(0)  # (1, D, H, W)
        
        # Prepare RoI boxes
        rois = []
        for box in boxes[i]:
            x1, y1, x2, y2 = box
            fx1 = x1 * feature_map_size
            fy1 = y1 * feature_map_size
            fx2 = x2 * feature_map_size
            fy2 = y2 * feature_map_size
            rois.append([0, fx1, fy1, fx2, fy2])
        
        rois_tensor = torch.tensor(rois, dtype=torch.float32, device=feat_map.device)
        
        # RoI Align
        pooled = roi_align(
            input=feat_map,
            boxes=rois_tensor,
            output_size=(1, 1),
            spatial_scale=1.0,
            sampling_ratio=-1,
            aligned=True,
        )
        
        region_feats = pooled.squeeze(-1).squeeze(-1)
        all_region_features.append(region_feats)
    
    return all_region_features
```

### 2. åŒºåŸŸå¯¹æ¯”æŸå¤±
```python
def compute_region_loss(self, ref_image_embeds, target_image_embeds, ref_boxes, target_boxes):
    """è®¡ç®—åŒºåŸŸçº§å¯¹æ¯”æŸå¤±"""
    ref_region_feats = self.extract_region_features(ref_image_embeds, ref_boxes)
    target_region_feats = self.extract_region_features(target_image_embeds, target_boxes)
    
    total_loss = 0.0
    valid_pairs = 0
    
    for i, (ref_feats, tgt_feats) in enumerate(zip(ref_region_feats, target_region_feats)):
        if ref_feats.shape[0] == 0 or tgt_feats.shape[0] == 0:
            continue
        
        # æŠ•å½±å¹¶å½’ä¸€åŒ–
        ref_proj = F.normalize(self.region_proj(ref_feats), dim=-1)
        tgt_proj = F.normalize(self.region_proj(tgt_feats), dim=-1)
        
        # è®¡ç®—ç›¸ä¼¼åº¦çŸ©é˜µ
        sim_matrix = torch.matmul(ref_proj, tgt_proj.t()) / self.temp
        
        # å¯¹æ¯”æŸå¤±
        if ref_proj.shape[0] == tgt_proj.shape[0]:
            labels = torch.arange(ref_proj.shape[0], device=sim_matrix.device)
            loss = F.cross_entropy(sim_matrix, labels)
        else:
            max_sim, _ = sim_matrix.max(dim=1)
            loss = -max_sim.mean()
        
        total_loss += loss
        valid_pairs += 1
    
    return total_loss / valid_pairs if valid_pairs > 0 else torch.tensor(0.0, device=ref_image_embeds.device)
```

## ğŸ“Š æ–‡ä»¶å˜æ›´ç»Ÿè®¡

| æ–‡ä»¶ | å˜æ›´ç±»å‹ | è¡Œæ•° | è¯´æ˜ |
|------|---------|------|------|
| `blip2_qformer_cir_align_prompt.py` | ä¿®æ”¹ | +130 | æ·»åŠ RoI AlignåŠŸèƒ½ |
| `blip_fine_tune_2.py` | ä¿®æ”¹ | +15 | æ·»åŠ è®­ç»ƒæ”¯æŒ |
| `ROI_ALIGN_USAGE.md` | æ–°å»º | +400 | ä½¿ç”¨æ–‡æ¡£ |
| `test_roi_align.py` | æ–°å»º | +350 | æµ‹è¯•è„šæœ¬ |
| `train_with_roi_align.sh` | æ–°å»º | +150 | è®­ç»ƒç¤ºä¾‹ |
| `ROI_ALIGN_IMPLEMENTATION_SUMMARY.md` | æ–°å»º | - | æœ¬æ–‡æ¡£ |

## ğŸš€ ä½¿ç”¨æ–¹æ³•

### å¿«é€Ÿå¼€å§‹

#### 1. è¿è¡Œæµ‹è¯•ï¼ˆéªŒè¯åŠŸèƒ½ï¼‰
```bash
cd /home/caoyu/mnt/zhaoai/SPRC
python test_roi_align.py
```

#### 2. æ ‡å‡†è®­ç»ƒï¼ˆä¸ä½¿ç”¨åŒºåŸŸæŸå¤±ï¼‰
```bash
cd /home/caoyu/mnt/zhaoai/SPRC/src
python blip_fine_tune_2.py \
    --dataset CIRR \
    --blip-model-name blip2_cir_align_prompt \
    --backbone pretrain \
    --num-epochs 50 \
    --batch-size 128 \
    --learning-rate 2e-6 \
    --loss-align 0.4 \
    --loss-rtc 0.4 \
    --save-training \
    --save-best
```

#### 3. å¯ç”¨åŒºåŸŸæŸå¤±è®­ç»ƒï¼ˆéœ€è¦boxæ•°æ®ï¼‰
```bash
cd /home/caoyu/mnt/zhaoai/SPRC/src
python blip_fine_tune_2.py \
    --dataset CIRR \
    --blip-model-name blip2_cir_align_prompt \
    --backbone pretrain \
    --num-epochs 50 \
    --batch-size 128 \
    --learning-rate 2e-6 \
    --loss-align 0.4 \
    --loss-rtc 0.4 \
    --use-region-loss \
    --loss-region 0.5 \
    --save-training \
    --save-best
```

## âš ï¸ å½“å‰é™åˆ¶å’Œå¾…å®Œæˆå·¥ä½œ

### å½“å‰çŠ¶æ€
âœ… **å·²å®Œæˆ**: 
- RoI Alignæ ¸å¿ƒåŠŸèƒ½å®ç°
- åŒºåŸŸæŸå¤±è®¡ç®—
- è®­ç»ƒè„šæœ¬é›†æˆ
- æµ‹è¯•å’Œæ–‡æ¡£

âš ï¸ **å¾…å®Œæˆ**:
- Bounding boxæ•°æ®å‡†å¤‡
- æ•°æ®åŠ è½½å™¨ä¿®æ”¹ï¼ˆæ”¯æŒåŠ è½½boxesï¼‰
- å®é™…è®­ç»ƒéªŒè¯

### ä¸‹ä¸€æ­¥å·¥ä½œ

#### 1. å‡†å¤‡Bounding Boxæ•°æ®
æœ‰ä¸‰ç§æ–¹å¼è·å–boxesï¼š

**æ–¹å¼A: ä½¿ç”¨ç›®æ ‡æ£€æµ‹æ¨¡å‹è‡ªåŠ¨ç”Ÿæˆ**
```python
# ä½¿ç”¨YOLOæˆ–å…¶ä»–æ£€æµ‹å™¨
from ultralytics import YOLO
model = YOLO('yolov8n.pt')

for image_path in image_paths:
    results = model(image_path)
    boxes = results[0].boxes.xyxyn  # å½’ä¸€åŒ–åæ ‡
    # ä¿å­˜boxes
```

**æ–¹å¼B: ä½¿ç”¨æ˜¾è‘—æ€§æ£€æµ‹**
```python
# ä½¿ç”¨æ˜¾è‘—æ€§æ£€æµ‹æ‰¾å…³é”®åŒºåŸŸ
import cv2
saliency = cv2.saliency.StaticSaliencySpectralResidual_create()
```

**æ–¹å¼C: æ‰‹åŠ¨æ ‡æ³¨**
- ä½¿ç”¨LabelImgç­‰å·¥å…·æ ‡æ³¨å…³é”®åŒºåŸŸ

#### 2. ä¿®æ”¹æ•°æ®åŠ è½½å™¨
åœ¨ `data_utils.py` ä¸­ä¿®æ”¹ `CIRRDataset`:

```python
class CIRRDataset(Dataset):
    def __init__(self, split, mode, preprocess, box_file=None):
        # ... åŸæœ‰ä»£ç  ...
        
        # åŠ è½½boxæ•°æ®
        self.boxes = {}
        if box_file and os.path.exists(box_file):
            with open(box_file, 'r') as f:
                self.boxes = json.load(f)
    
    def __getitem__(self, index):
        if self.mode == 'relative' and self.split == 'train':
            # ... åŸæœ‰ä»£ç  ...
            ref_boxes = self.boxes.get(reference_name, [])
            tgt_boxes = self.boxes.get(target_hard_name, [])
            return reference_image, target_image, rel_caption, ref_boxes, tgt_boxes
```

#### 3. ä¿®æ”¹è®­ç»ƒå¾ªç¯
åœ¨ `blip_fine_tune_2.py` ä¸­æ›´æ–°æ•°æ®å¤„ç†:

```python
for idx, batch_data in enumerate(train_bar):
    if len(batch_data) == 5:
        reference_images, target_images, captions, ref_boxes, tgt_boxes = batch_data
    else:
        reference_images, target_images, captions = batch_data
        ref_boxes, tgt_boxes = None, None
    
    # ... å…¶ä»–ä»£ç  ...
    
    loss_dict = blip_model({
        "image": reference_images,
        "target": target_images,
        "text_input": captions,
        "region_boxes": ref_boxes,
        "target_region_boxes": tgt_boxes
    })
```

## ğŸ” æŠ€æœ¯ç»†èŠ‚

### RoI Alignå·¥ä½œåŸç†
1. **è¾“å…¥**: ViTçš„patchç‰¹å¾ (B, 196, 768) å¯¹äº224x224å›¾åƒ
2. **Reshape**: é‡æ„ä¸ºfeature map (B, 768, 14, 14)
3. **Boxè½¬æ¢**: å°†å½’ä¸€åŒ–åæ ‡[0,1]è½¬æ¢ä¸ºfeature mapåæ ‡[0,14]
4. **RoI Align**: ä½¿ç”¨åŒçº¿æ€§æ’å€¼æå–å›ºå®šå¤§å°çš„åŒºåŸŸç‰¹å¾
5. **è¾“å‡º**: æ¯ä¸ªboxçš„ç‰¹å¾å‘é‡ (num_boxes, 768)

### æŸå¤±å‡½æ•°è®¾è®¡
```
Total Loss = loss_itc + 
             loss_rtc * weight_rtc + 
             loss_align * weight_align + 
             loss_region * weight_region
```

é»˜è®¤æƒé‡:
- `loss_itc`: 1.0 (å›ºå®š)
- `loss_rtc`: 0.4
- `loss_align`: 0.4
- `loss_region`: 0.5

## ğŸ“ˆ é¢„æœŸæ•ˆæœ

### æ€§èƒ½æå‡
- **ç»†ç²’åº¦æ£€ç´¢**: +2-5% Recall@K
- **å±æ€§ç†è§£**: æ›´å¥½çš„é¢œè‰²ã€å½¢çŠ¶è¯†åˆ«
- **ç©ºé—´å…³ç³»**: æ”¹å–„ä½ç½®ç†è§£

### é€‚ç”¨åœºæ™¯
- âœ… CIRRæ•°æ®é›†
- âœ… éœ€è¦ç»†ç²’åº¦ç†è§£çš„ä»»åŠ¡
- âœ… æœ‰å±€éƒ¨æ ‡æ³¨çš„æ•°æ®é›†

## ğŸ› è°ƒè¯•å»ºè®®

### å¦‚æœé‡åˆ°é—®é¢˜

1. **è¿è¡Œæµ‹è¯•è„šæœ¬**
```bash
python test_roi_align.py
```

2. **æ£€æŸ¥boxesæ ¼å¼**
- å¿…é¡»æ˜¯å½’ä¸€åŒ–åæ ‡ [0, 1]
- æ ¼å¼: [x1, y1, x2, y2]

3. **æ£€æŸ¥å†…å­˜ä½¿ç”¨**
- RoI Alignä¼šå¢åŠ å†…å­˜æ¶ˆè€—
- å¯èƒ½éœ€è¦å‡å°batch size

4. **æŸ¥çœ‹æŸå¤±å€¼**
- å¦‚æœloss_regionä¸º0ï¼Œæ£€æŸ¥boxesæ˜¯å¦æ­£ç¡®ä¼ é€’
- å¦‚æœloss_regionè¿‡å¤§ï¼Œé™ä½weight

## ğŸ“š å‚è€ƒèµ„æ–™

- **FG-CLIPè®ºæ–‡**: https://arxiv.org/abs/2505.05071
- **FG-CLIPä»£ç **: /home/caoyu/mnt/zhaoai/FG-CLIP
- **RoI Alignè®ºæ–‡**: https://arxiv.org/abs/1703.06870
- **CIRRæ•°æ®é›†**: https://arxiv.org/abs/2104.03015

## âœ¨ æ€»ç»“

å·²æˆåŠŸå®ç°æ–¹æ¡ˆ1ï¼ˆæœ€å°ä¾µå…¥å¼é›†æˆï¼‰ï¼Œæ ¸å¿ƒåŠŸèƒ½åŒ…æ‹¬ï¼š

âœ… **å®Œæˆçš„åŠŸèƒ½**:
1. RoI AlignåŒºåŸŸç‰¹å¾æå–
2. åŒºåŸŸçº§å¯¹æ¯”æŸå¤±
3. è®­ç»ƒè„šæœ¬é›†æˆ
4. å®Œæ•´çš„æµ‹è¯•å¥—ä»¶
5. è¯¦ç»†çš„æ–‡æ¡£

ğŸ¯ **ä¸‹ä¸€æ­¥**:
1. å‡†å¤‡bounding boxæ•°æ®
2. ä¿®æ”¹æ•°æ®åŠ è½½å™¨
3. è¿è¡Œå®é™…è®­ç»ƒ
4. è¯„ä¼°æ€§èƒ½æå‡

ğŸ’¡ **å»ºè®®**:
- å…ˆè¿è¡Œæµ‹è¯•éªŒè¯åŠŸèƒ½
- ä½¿ç”¨ä¸¤é˜¶æ®µè®­ç»ƒç­–ç•¥
- æ ¹æ®å®é™…æ•ˆæœè°ƒæ•´æƒé‡

---

**å®ç°æ—¥æœŸ**: 2025-11-18  
**å®ç°è€…**: AI Assistant  
**ç‰ˆæœ¬**: v1.0
