# RoI AlignåŒºåŸŸç‰¹å¾æå–æ¨¡å—ä½¿ç”¨æŒ‡å—

## ğŸ“‹ æ¦‚è¿°

æœ¬æ–‡æ¡£è¯´æ˜å¦‚ä½•ä½¿ç”¨æ–°æ·»åŠ çš„RoI AlignåŒºåŸŸç‰¹å¾æå–åŠŸèƒ½ï¼Œè¯¥åŠŸèƒ½ä»FG-CLIPç§»æ¤è€Œæ¥ï¼Œç”¨äºæå‡CIRRæ•°æ®é›†ä¸Šçš„ç»†ç²’åº¦æ£€ç´¢æ€§èƒ½ã€‚

## ğŸ¯ åŠŸèƒ½è¯´æ˜

### æ ¸å¿ƒåŠŸèƒ½
- **åŒºåŸŸç‰¹å¾æå–**: ä½¿ç”¨RoI Alignä»å›¾åƒçš„å¯†é›†ç‰¹å¾å›¾ä¸­æå–æŒ‡å®šåŒºåŸŸçš„ç‰¹å¾
- **åŒºåŸŸçº§å¯¹æ¯”æŸå¤±**: è®¡ç®—å‚è€ƒå›¾åƒå’Œç›®æ ‡å›¾åƒå¯¹åº”åŒºåŸŸä¹‹é—´çš„å¯¹æ¯”æŸå¤±
- **çµæ´»æ§åˆ¶**: å¯ä»¥é€šè¿‡å‘½ä»¤è¡Œå‚æ•°å¯ç”¨/ç¦ç”¨è¯¥åŠŸèƒ½

### å®ç°ä½ç½®
- **æ¨¡å‹æ–‡ä»¶**: `/home/caoyu/mnt/zhaoai/SPRC/src/lavis/models/blip2_models/blip2_qformer_cir_align_prompt.py`
- **è®­ç»ƒè„šæœ¬**: `/home/caoyu/mnt/zhaoai/SPRC/src/blip_fine_tune_2.py`

## ğŸš€ ä½¿ç”¨æ–¹æ³•

### æ–¹æ³•1: ä¸ä½¿ç”¨åŒºåŸŸæŸå¤±ï¼ˆé»˜è®¤ï¼‰

```bash
cd /home/caoyu/mnt/zhaoai/SPRC/src

python blip_fine_tune_2.py \
    --dataset CIRR \
    --blip-model-name blip2_cir_align_prompt \
    --backbone pretrain \
    --num-epochs 50 \
    --batch-size 128 \
    --learning-rate 2e-6 \
    --loss-align 0.4 \
    --loss-rtc 0.4 \
    --save-training \
    --save-best
```

### æ–¹æ³•2: å¯ç”¨åŒºåŸŸæŸå¤±ï¼ˆéœ€è¦bounding boxæ•°æ®ï¼‰

```bash
cd /home/caoyu/mnt/zhaoai/SPRC/src

python blip_fine_tune_2.py \
    --dataset CIRR \
    --blip-model-name blip2_cir_align_prompt \
    --backbone pretrain \
    --num-epochs 50 \
    --batch-size 128 \
    --learning-rate 2e-6 \
    --loss-align 0.4 \
    --loss-rtc 0.4 \
    --use-region-loss \
    --loss-region 0.5 \
    --save-training \
    --save-best
```

### å‚æ•°è¯´æ˜

| å‚æ•° | ç±»å‹ | é»˜è®¤å€¼ | è¯´æ˜ |
|------|------|--------|------|
| `--use-region-loss` | flag | False | æ˜¯å¦å¯ç”¨RoI AlignåŒºåŸŸæŸå¤± |
| `--loss-region` | float | 0.5 | åŒºåŸŸæŸå¤±çš„æƒé‡ |
| `--loss-align` | float | 0.4 | å¯¹é½æŸå¤±çš„æƒé‡ |
| `--loss-rtc` | float | 0.4 | ç›¸å¯¹å¯¹æ¯”æŸå¤±çš„æƒé‡ |

## ğŸ“ ä»£ç é›†æˆè¯´æ˜

### 1. æ¨¡å‹ä¿®æ”¹

åœ¨ `blip2_qformer_cir_align_prompt.py` ä¸­æ·»åŠ äº†ä»¥ä¸‹åŠŸèƒ½ï¼š

#### a) RoI Alignç‰¹å¾æå–
```python
def extract_region_features(self, image_embeds, boxes, image_size=(224, 224)):
    """
    ä½¿ç”¨RoI Alignä»å›¾åƒç‰¹å¾ä¸­æå–åŒºåŸŸç‰¹å¾
    
    Args:
        image_embeds: å›¾åƒç‰¹å¾ (B, N, D) å…¶ä¸­Næ˜¯patchæ•°é‡
        boxes: bounding boxesåˆ—è¡¨ï¼Œæ¯ä¸ªå…ƒç´ æ˜¯è¯¥å›¾åƒçš„boxes (x1, y1, x2, y2)æ ¼å¼ï¼Œå½’ä¸€åŒ–åˆ°[0,1]
    
    Returns:
        region_features: åŒºåŸŸç‰¹å¾åˆ—è¡¨
    """
```

#### b) åŒºåŸŸçº§å¯¹æ¯”æŸå¤±
```python
def compute_region_loss(self, ref_image_embeds, target_image_embeds, ref_boxes, target_boxes):
    """
    è®¡ç®—åŒºåŸŸçº§å¯¹æ¯”æŸå¤±
    
    Args:
        ref_image_embeds: å‚è€ƒå›¾åƒç‰¹å¾
        target_image_embeds: ç›®æ ‡å›¾åƒç‰¹å¾
        ref_boxes: å‚è€ƒå›¾åƒçš„bounding boxes
        target_boxes: ç›®æ ‡å›¾åƒçš„bounding boxes
    
    Returns:
        loss_region: åŒºåŸŸçº§å¯¹æ¯”æŸå¤±
    """
```

### 2. å‰å‘ä¼ æ’­ä¿®æ”¹

åœ¨ `forward` æ–¹æ³•ä¸­ï¼š
```python
def forward(self, samples):
    # ... åŸæœ‰çš„æŸå¤±è®¡ç®— ...
    
    losses = {
        'loss_itc': loss_itc, 
        'loss_rtc': loss_rtc,
        'loss_align': loss_align
    }
    
    # å¦‚æœæä¾›äº†region boxesï¼Œè®¡ç®—åŒºåŸŸçº§æŸå¤±
    if self.use_region_loss and 'region_boxes' in samples and samples['region_boxes'] is not None:
        loss_region = self.compute_region_loss(
            image_embeds, target_embeds, 
            samples['region_boxes'], samples.get('target_region_boxes')
        )
        losses['loss_region'] = loss_region
    
    return losses
```

## ğŸ”§ å¦‚ä½•æ·»åŠ Bounding Boxæ•°æ®

### å½“å‰çŠ¶æ€
ç›®å‰ä»£ç ä¸­ `region_boxes` è®¾ç½®ä¸º `None`ï¼Œå› æ­¤åŒºåŸŸæŸå¤±ä¸ä¼šè¢«è®¡ç®—ã€‚

### æ·»åŠ Boxæ•°æ®çš„æ­¥éª¤

#### æ­¥éª¤1: å‡†å¤‡Boxæ ‡æ³¨æ•°æ®

åˆ›å»ºä¸€ä¸ªJSONæ–‡ä»¶ï¼Œæ ¼å¼å¦‚ä¸‹ï¼š
```json
{
    "image_name_1": [
        [x1, y1, x2, y2],  // ç¬¬ä¸€ä¸ªboxï¼Œå½’ä¸€åŒ–åˆ°[0,1]
        [x1, y1, x2, y2]   // ç¬¬äºŒä¸ªbox
    ],
    "image_name_2": [
        [x1, y1, x2, y2]
    ]
}
```

#### æ­¥éª¤2: ä¿®æ”¹æ•°æ®åŠ è½½å™¨

åœ¨ `data_utils.py` çš„ `CIRRDataset` ç±»ä¸­æ·»åŠ boxåŠ è½½ï¼š

```python
class CIRRDataset(Dataset):
    def __init__(self, split: str, mode: str, preprocess: callable, box_file: str = None):
        # ... åŸæœ‰ä»£ç  ...
        
        # åŠ è½½boxæ•°æ®
        self.boxes = {}
        if box_file and os.path.exists(box_file):
            with open(box_file, 'r') as f:
                self.boxes = json.load(f)
    
    def __getitem__(self, index):
        if self.mode == 'relative' and self.split == 'train':
            # ... åŸæœ‰ä»£ç  ...
            
            # è·å–boxes
            ref_boxes = self.boxes.get(reference_name, [])
            tgt_boxes = self.boxes.get(target_hard_name, [])
            
            return reference_image, target_image, rel_caption, ref_boxes, tgt_boxes
```

#### æ­¥éª¤3: ä¿®æ”¹è®­ç»ƒå¾ªç¯

åœ¨ `blip_fine_tune_2.py` ä¸­ï¼š

```python
for idx, batch_data in enumerate(train_bar):
    if len(batch_data) == 5:
        reference_images, target_images, captions, ref_boxes, tgt_boxes = batch_data
    else:
        reference_images, target_images, captions = batch_data
        ref_boxes, tgt_boxes = None, None
    
    # ... å…¶ä»–ä»£ç  ...
    
    loss_dict = blip_model({
        "image": reference_images, 
        "target": target_images, 
        "text_input": captions,
        "region_boxes": ref_boxes,
        "target_region_boxes": tgt_boxes
    })
```

## ğŸ“Š é¢„æœŸæ•ˆæœ

### æ€§èƒ½æå‡
- **ç»†ç²’åº¦æ£€ç´¢**: é€šè¿‡å…³æ³¨å±€éƒ¨åŒºåŸŸï¼Œæå‡å¯¹ç»†å¾®å·®å¼‚çš„è¯†åˆ«èƒ½åŠ›
- **å±æ€§ç†è§£**: æ›´å¥½åœ°ç†è§£é¢œè‰²ã€å½¢çŠ¶ç­‰å±€éƒ¨å±æ€§
- **ç©ºé—´å…³ç³»**: æ”¹å–„å¯¹ç‰©ä½“ä½ç½®å’Œç©ºé—´å…³ç³»çš„ç†è§£

### é€‚ç”¨åœºæ™¯
- CIRRæ•°æ®é›†çš„è®­ç»ƒå’Œè¯„ä¼°
- éœ€è¦ç»†ç²’åº¦ç†è§£çš„å›¾åƒæ£€ç´¢ä»»åŠ¡
- æœ‰bounding boxæ ‡æ³¨çš„æ•°æ®é›†

## âš ï¸ æ³¨æ„äº‹é¡¹

### 1. å†…å­˜æ¶ˆè€—
- RoI Alignä¼šå¢åŠ ä¸€å®šçš„å†…å­˜æ¶ˆè€—
- å»ºè®®æ ¹æ®GPUå†…å­˜è°ƒæ•´batch size

### 2. Boxæ•°æ®æ ¼å¼
- Boxeså¿…é¡»å½’ä¸€åŒ–åˆ°[0, 1]èŒƒå›´
- æ ¼å¼ä¸º `[x1, y1, x2, y2]`ï¼Œå…¶ä¸­ `(x1, y1)` æ˜¯å·¦ä¸Šè§’ï¼Œ`(x2, y2)` æ˜¯å³ä¸‹è§’

### 3. è®­ç»ƒæ—¶é—´
- å¯ç”¨åŒºåŸŸæŸå¤±ä¼šç•¥å¾®å¢åŠ è®­ç»ƒæ—¶é—´ï¼ˆçº¦10-15%ï¼‰

### 4. æ¸è¿›å¼è®­ç»ƒç­–ç•¥
å»ºè®®é‡‡ç”¨ä¸¤é˜¶æ®µè®­ç»ƒï¼š
1. **ç¬¬ä¸€é˜¶æ®µ**: ä¸ä½¿ç”¨åŒºåŸŸæŸå¤±ï¼Œè®­ç»ƒå…¨å±€ç‰¹å¾ï¼ˆ20-30 epochsï¼‰
2. **ç¬¬äºŒé˜¶æ®µ**: å¯ç”¨åŒºåŸŸæŸå¤±ï¼Œå¾®è°ƒç»†ç²’åº¦ç‰¹å¾ï¼ˆ10-20 epochsï¼‰

## ğŸ§ª æµ‹è¯•ä»£ç 

åˆ›å»ºä¸€ä¸ªç®€å•çš„æµ‹è¯•è„šæœ¬éªŒè¯åŠŸèƒ½ï¼š

```python
import torch
from lavis.models import load_model_and_preprocess

# åŠ è½½æ¨¡å‹
model, vis_processors, txt_processors = load_model_and_preprocess(
    name="blip2_cir_align_prompt", 
    model_type="pretrain", 
    is_eval=False, 
    device="cuda"
)

# å¯ç”¨åŒºåŸŸæŸå¤±
model.use_region_loss = True

# åˆ›å»ºæµ‹è¯•æ•°æ®
batch_size = 2
image = torch.randn(batch_size, 3, 224, 224).cuda()
target = torch.randn(batch_size, 3, 224, 224).cuda()
text = ["a red car", "a blue shirt"]

# æµ‹è¯•boxesï¼ˆå½’ä¸€åŒ–åæ ‡ï¼‰
ref_boxes = [
    [[0.1, 0.1, 0.5, 0.5], [0.6, 0.6, 0.9, 0.9]],  # ç¬¬ä¸€å¼ å›¾çš„2ä¸ªboxes
    [[0.2, 0.2, 0.8, 0.8]]  # ç¬¬äºŒå¼ å›¾çš„1ä¸ªbox
]
tgt_boxes = [
    [[0.15, 0.15, 0.55, 0.55], [0.65, 0.65, 0.95, 0.95]],
    [[0.25, 0.25, 0.85, 0.85]]
]

# å‰å‘ä¼ æ’­
samples = {
    "image": image,
    "target": target,
    "text_input": text,
    "region_boxes": ref_boxes,
    "target_region_boxes": tgt_boxes
}

losses = model(samples)
print("Losses:", losses)
```

## ğŸ“š å‚è€ƒèµ„æ–™

- **FG-CLIPè®ºæ–‡**: [FG-CLIP: Fine-Grained Visual and Textual Alignment](https://arxiv.org/abs/2505.05071)
- **RoI Align**: [Mask R-CNN](https://arxiv.org/abs/1703.06870)
- **CIRRæ•°æ®é›†**: [Composed Image Retrieval using Contrastive Learning](https://arxiv.org/abs/2104.03015)

## ğŸ¤ è´¡çŒ®

å¦‚æœä½ å‘ç°bugæˆ–æœ‰æ”¹è¿›å»ºè®®ï¼Œè¯·ï¼š
1. æ£€æŸ¥ä»£ç å®ç°
2. è¿è¡Œæµ‹è¯•
3. æäº¤issueæˆ–pull request

## ğŸ“ è”ç³»æ–¹å¼

å¦‚æœ‰é—®é¢˜ï¼Œè¯·æŸ¥çœ‹ï¼š
- ä»£ç æ³¨é‡Š
- æœ¬æ–‡æ¡£
- FG-CLIPåŸå§‹å®ç°
